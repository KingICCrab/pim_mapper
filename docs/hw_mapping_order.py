#!/usr/bin/env python3
"""
PE Array H/W 维度映射 - 包含维度顺序的影响
"""

print("""
================================================================================
        PE Array H/W 维度映射 - 维度顺序的意义
================================================================================

当多个维度映射到同一方向时，顺序很重要！

记法: H = [dim_inner, dim_outer]
  • dim_inner: 内层维度，相邻PE处理连续的inner值
  • dim_outer: 外层维度，每隔inner_size个PE才变化

例: H = [K, C], H_size = 16, K=4, C=4
    PE排列: PE[0]=(K=0,C=0), PE[1]=(K=1,C=0), PE[2]=(K=2,C=0), PE[3]=(K=3,C=0),
            PE[4]=(K=0,C=1), PE[5]=(K=1,C=1), ...

================================================================================
                    维度顺序对数据通信的影响
================================================================================

┌─────────────────────────────────────────────────────────────────────────────┐
│  H = [K_inner, C_outer]  vs  H = [C_inner, K_outer]                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  H = [K, C] (K在内层):                                                      │
│  ┌────┬────┬────┬────┬────┬────┬────┬────┐                                 │
│  │K=0 │K=1 │K=2 │K=3 │K=0 │K=1 │K=2 │K=3 │                                 │
│  │C=0 │C=0 │C=0 │C=0 │C=1 │C=1 │C=1 │C=1 │                                 │
│  └────┴────┴────┴────┴────┴────┴────┴────┘                                 │
│  │←── K 近邻 ──→│    │←── K 近邻 ──→│                                       │
│                                                                             │
│  → Weight (与K相关): 相邻PE用不同Weight，近邻通信                            │
│  → C方向Reduction: 需要跨越K_size个PE才能累加，远距离通信                    │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  H = [C, K] (C在内层):                                                      │
│  ┌────┬────┬────┬────┬────┬────┬────┬────┐                                 │
│  │C=0 │C=1 │C=2 │C=3 │C=0 │C=1 │C=2 │C=3 │                                 │
│  │K=0 │K=0 │K=0 │K=0 │K=1 │K=1 │K=1 │K=1 │                                 │
│  └────┴────┴────┴────┴────┴────┴────┴────┘                                 │
│  │←── C 近邻 ──→│    │←── C 近邻 ──→│                                       │
│                                                                             │
│  → C方向Reduction: 相邻PE的psum可以累加，近邻通信 ✓                          │
│  → Weight (与K相关): 每K_size个PE共享Weight，可以广播                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

结论: 如果需要Reduction，把Reduction维度放在内层更高效！

================================================================================
                    完整的 H/W 映射表（含顺序）
================================================================================

记法: H=[a,b] 表示 a 是内层，b 是外层
      W=[c,d] 表示 c 是内层，d 是外层

--------------------------------------------------------------------------------
  H 方向          |  W 方向        | Reduction    | Dataflow     | 计算单元
--------------------------------------------------------------------------------
  [K]             |  [P,Q] 或 [Q,P]| None         | Weight Stat. | Scalar PE
  [P,Q] 或 [Q,P]  |  [K]           | None         | Output Stat. | Scalar PE
--------------------------------------------------------------------------------
  [K]             |  [C]           | W方向(近邻)  | Tensor Core  | NVIDIA TC
                  |                | 内部树累加   | 风格         | (C在内部)
--------------------------------------------------------------------------------
  [C]             |  [K]           | H方向(近邻)  | Input Stat.  | 需要H方向
                  |                |              |              | reduction网络
--------------------------------------------------------------------------------
  [C,K]           |  [P,Q]         | H方向        | Mixed        | C在内层
  (C内层)         |                | C近邻累加 ✓  |              | 利于reduction
--------------------------------------------------------------------------------
  [K,C]           |  [P,Q]         | H方向        | Mixed        | C在外层
  (K内层)         |                | C远距累加 ✗  |              | reduction低效
--------------------------------------------------------------------------------
  [R,S]           |  [K]           | H方向        | Row Stat.    | Eyeriss
  (R内层)         |                | R×S近邻累加  |              |
--------------------------------------------------------------------------------
  [K]             |  [C]           | Systolic     | TPU Systolic | Google TPU
  (systolic行)    |  (systolic列)  | 沿H方向流动  |              |
--------------------------------------------------------------------------------

================================================================================
                    Systolic Array 的顺序特殊性
================================================================================

TPU Systolic Array 中，顺序决定了数据流动方向：

┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   Weight Stationary Systolic:                                               │
│                                                                             │
│        Activation 沿 W 方向流入 (C 维度)                                    │
│        ─────────────────────────────→                                       │
│                                                                             │
│      W方向 = [C] (C是W的唯一维度)                                           │
│      ┌────┬────┬────┬────┐                                                 │
│   ↓  │W   │W   │W   │W   │  H方向 = [K] (K是H的唯一维度)                   │
│   P  │K=0 │K=0 │K=0 │K=0 │                                                 │
│   s  │C=0 │C=1 │C=2 │C=3 │                                                 │
│   u  ├────┼────┼────┼────┤                                                 │
│   m  │W   │W   │W   │W   │                                                 │
│   沿 │K=1 │K=1 │K=1 │K=1 │                                                 │
│   H  │C=0 │C=1 │C=2 │C=3 │                                                 │
│   方 └────┴────┴────┴────┘                                                 │
│   向      ↓    ↓    ↓    ↓                                                  │
│   流      Psum 向下传递，累加不同 K 的贡献                                   │
│   出                                                                        │
│                                                                             │
│   数据流顺序:                                                               │
│   1. Activation[C=0] 从左进入第0列，同时向下传播                            │
│   2. Activation[C=0] 移到第1列，Activation[C=1] 进入第0列                   │
│   3. Psum 沿 H 方向（K方向）累加                                            │
│                                                                             │
│   关键: W方向是C（Activation流动），H方向是K（Psum累加）                    │
│         顺序是固定的，由 systolic 结构决定                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
                    Tensor Core 的内部顺序
================================================================================

Tensor Core 的 Reduction 维度在内部，外部只看到 Output 维度：

┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   外部视角 (H/W 映射):                                                      │
│     H = [K] (或 [K,P] 等 Output 维度组合)                                   │
│     W = [N] (或 [Q,N] 等 Output 维度组合)                                   │
│     → 没有 Reduction 维度暴露在外部！                                       │
│                                                                             │
│   内部视角 (每个 PE 内部):                                                   │
│     每个 PE 接收 K_red = 16 个元素进行 reduction                            │
│     └─→ 16 个乘法 + 16-way 加法树                                          │
│                                                                             │
│   所以 Tensor Core 的特点:                                                  │
│     • H/W 只能映射 Output 相关维度 (K, P, Q, N)                             │
│     • Reduction 维度 (C, R, S) 被"吸收"到 PE 内部                           │
│     • 外部看不到 Reduction，所以不需要考虑 Reduction 维度的顺序             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
                    ILP 建模：维度顺序的变量
================================================================================

方法1: 用 order 变量显式建模顺序

  order_h[j1, j2] ∈ {0, 1}  : j1 是否在 j2 的内层 (H方向)
  order_w[j1, j2] ∈ {0, 1}  : j1 是否在 j2 的内层 (W方向)
  
  约束: order 是严格的偏序关系
  
方法2: 用 position 变量

  pos_h[j] ∈ {0, 1, ..., num_dims_in_h - 1}  : 维度 j 在 H 方向的位置
  pos_w[j] ∈ {0, 1, ..., num_dims_in_w - 1}  : 维度 j 在 W 方向的位置
  
  位置越小 = 越内层

方法3: 简化 - 只关心 Reduction 维度的位置

  red_inner_h ∈ {0, 1}  : Reduction 维度是否在 H 方向的内层
  red_inner_w ∈ {0, 1}  : Reduction 维度是否在 W 方向的内层
  
  如果 red_inner_h = 1，则 Reduction 可以用近邻通信，成本低
  如果 red_inner_h = 0，则 Reduction 需要远距通信，成本高

================================================================================
                    顺序对通信成本的影响
================================================================================

假设 H = [dim_inner, dim_outer]，inner_size = 4, outer_size = 4

如果需要在 dim_inner 方向做 Reduction:
  → 相邻 PE 累加，通信距离 = 1
  → Reduction 延迟 = log2(inner_size) × 近邻延迟

如果需要在 dim_outer 方向做 Reduction:
  → PE[i] 和 PE[i + inner_size] 累加，通信距离 = inner_size
  → Reduction 延迟 = log2(outer_size) × 远距延迟

成本比较:
  近邻 Reduction 成本 ≈ log2(N) × 1
  远距 Reduction 成本 ≈ log2(N) × inner_size  (可能更高)

所以: 把需要 Reduction 的维度放在内层是更优的选择！
""")
