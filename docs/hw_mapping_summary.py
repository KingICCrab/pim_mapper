#!/usr/bin/env python3
"""
PE Array H/W 维度映射空间 → Dataflow → 计算单元类型
包含维度顺序的影响
"""

print("""
================================================================================
             PE Array H/W 维度映射 → Dataflow → 计算单元类型
================================================================================

卷积: Output[N,K,P,Q] = Σ_{r,s,c} Input[N,C,P+r,Q+s] × Weight[K,C,R,S]
  • Output相关维度: K, P, Q, N  (并行时无需reduction)
  • Reduction维度:  R, S, C     (并行时需要累加partial sum)

记法: [a, b] 表示 a 是内层(相邻PE), b 是外层(跨块PE)
      内层维度的相邻PE可以近邻通信，外层需要远距通信

================================================================================
                        完整映射表（含维度顺序）
================================================================================
  H 方向        |  W 方向       | Reduction      | Dataflow       | 计算单元
--------------------------------------------------------------------------------
  [K]           |  [P, Q]       | None           | Weight Stat.   | Scalar PE
  [K]           |  [Q, P]       | None           | Weight Stat.   | Scalar PE
  [P, Q]        |  [K]          | None           | Output Stat.   | ShiDianNao
  [Q, P]        |  [K]          | None           | Output Stat.   | Scalar PE
--------------------------------------------------------------------------------
  [K]           |  [C] (内部)   | W内部树        | ★Tensor Core   | NVIDIA TC
  [P]           |  [C] (内部)   | W内部树        | ★Tensor Core   | (C在PE内部)
--------------------------------------------------------------------------------
  [C]           |  [K]          | H近邻 ✓        | Input Stat.    | 需H-reduction
  [K]           |  [C]          | W近邻 ✓        | Input Stat.    | 需W-reduction
--------------------------------------------------------------------------------
  [K]sys        |  [C]sys       | Systolic流     | ★TPU Systolic  | Google TPU
                |               | (H方向传psum)  |                |
--------------------------------------------------------------------------------
  [R, S]        |  [K]          | H近邻(R×S) ✓   | Row Stat.      | Eyeriss
  [S, R]        |  [K]          | H近邻(R×S) ✓   | Row Stat.      | Eyeriss
  [R, S]        |  [P, Q]       | H近邻(R×S) ✓   | Row Stat.      | Eyeriss v2
--------------------------------------------------------------------------------
  [C, K]        |  [P, Q]       | H近邻(C) ✓     | Mixed          | C内层,高效
  [K, C]        |  [P, Q]       | H远距(C) ✗     | Mixed          | C外层,低效
--------------------------------------------------------------------------------
  [C, K]        |  [C, P]       | H近邻+W近邻    | 2D Reduction   | 特殊硬件
  [K, C]        |  [P, C]       | H远距+W远距 ✗  | 2D Reduction   | 极低效
--------------------------------------------------------------------------------

================================================================================
                           Tensor Core vs TPU Systolic 对比
================================================================================

┌─────────────────────────────────────────────────────────────────────────────┐
│                        Tensor Core (16×16×16)                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   矩阵乘: C[M×N] = A[M×K] × B[K×N]                                          │
│                                                                             │
│   H/W 映射:                                                                 │
│     • H方向 = M = K (output channels)        → 16 rows                     │
│     • W方向 = N = P×Q (output spatial)       → 16 cols                     │
│     • 内部  = K_red = C (input channels)     → 16-way reduction tree       │
│                                                                             │
│   每个PE的计算:                                                             │
│     C[i,j] = Σ_{k=0}^{15} A[i,k] × B[k,j]                                  │
│     └─── 16个乘法 + 16-way加法树 → 1个结果 (1 cycle, pipelined)            │
│                                                                             │
│   数据流:                                                                   │
│     • A (Weight): 沿H方向分布, W方向广播 (每行相同)                         │
│     • B (Input):  沿W方向分布, H方向广播 (每列相同)                         │
│     • C (Output): 每个PE累加到自己的位置                                    │
│                                                                             │
│   关键: Reduction在PE内部完成, 对外是 Output Stationary                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                        TPU Systolic Array (128×128)                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   H/W 映射:                                                                 │
│     • H方向 = K (output channels)            → 128 rows                    │
│     • W方向 = C (input channels, Reduction!) → 128 cols                    │
│                                                                             │
│   数据流:                                                                   │
│     • Weight: 预加载到每个PE, Weight[K=i, C=j] 在 PE[i,j]                   │
│     • Activation: 从左边流入, 沿W方向 systolic 传递                         │
│     • Psum: 从上往下流出, 沿H方向 systolic 累加                             │
│                                                                             │
│        Act →  [W00]→[W01]→[W02]→...                                        │
│               ↓     ↓     ↓                                                 │
│        Act →  [W10]→[W11]→[W12]→...                                        │
│               ↓     ↓     ↓                                                 │
│               ↓     ↓     ↓                                                 │
│              Out   Out   Out   (累加后的结果)                               │
│                                                                             │
│   关键: Reduction通过PE间Systolic传递, 延迟=H_size cycles (可pipeline)      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
                     不同计算单元对 H/W 映射的约束
================================================================================

┌─────────────────────────────────────────────────────────────────────────────┐
│ 计算单元类型     │ H方向约束              │ W方向约束              │ Reduction │
├─────────────────────────────────────────────────────────────────────────────┤
│                  │                        │                        │          │
│ Scalar PE        │ 任意维度组合           │ 任意维度组合           │ Buffer/  │
│ (最灵活)         │ ∏xb_h[j] ≤ H_size      │ ∏xb_w[j] ≤ W_size      │ PE网络   │
│                  │                        │                        │          │
├─────────────────────────────────────────────────────────────────────────────┤
│                  │                        │                        │          │
│ Systolic Array   │ 必须: K 映射到 H       │ 必须: C 映射到 W       │ Systolic │
│ (TPU风格)        │ xb_h[K] = H_size       │ xb_w[C] = W_size       │ Flow     │
│                  │ (固定Weight Stationary)│ (C是Reduction维度)     │ H_size   │
│                  │                        │                        │ cycles   │
├─────────────────────────────────────────────────────────────────────────────┤
│                  │                        │                        │          │
│ Tensor Core      │ 只能用Output相关维度   │ 只能用Output相关维度   │ 内部树   │
│ (GPU风格)        │ xb_h[j], j∈{K,P,Q,N}   │ xb_w[j], j∈{K,P,Q,N}   │ 1 cycle  │
│                  │ (固定Output Stationary)│ Reduction维度在内部    │ (pipe-   │
│                  │                        │ C ≤ internal_red_dim   │ lined)   │
│                  │                        │                        │          │
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
                              ILP 建模总结
================================================================================

给定计算单元类型, ILP需要:

1. Scalar PE:
   - 变量: xb_h[j], xb_w[j] 对所有7个维度 j ∈ {R,S,P,Q,C,K,N}
   - 约束: ∏_j xb_h[j] ≤ H_size, ∏_j xb_w[j] ≤ W_size
   - 目标: 包含 reduction 代价 (如果xb_h/xb_w在R,S,C上>1)

2. Systolic Array:
   - 变量: xb_h[j], xb_w[j], 但 K 和 C 被固定
   - 约束: xb_h[K] = H_size, xb_w[C] = W_size
   - 目标: 包含 systolic pipeline 延迟 = H_size

3. Tensor Core:
   - 变量: xb_h[j], xb_w[j], 只对 j ∈ {K,P,Q,N}
   - 约束: xb_h[R,S,C] = 1, xb_w[R,S,C] = 1 (不能并行reduction维度)
   - 约束: workload.C ≤ internal_reduction_dim (或需要多次调用)
   - 目标: reduction 延迟 = 1 (内部完成)
""")
