% !TeX root = global_partition/docs/global_partition_paper.tex
\subsection{NN Partitioning across Processing Elements}
\label{sec:nn-partition}

The processing element (PE) array contains multiple PEs, each with separate local memory. In addition to processing different NNs or layers in each PE, we can divide large NN layers across the PEs to process them in parallel. We first present a taxonomy of the different partitioning schemes summarized in Figure~\ref{fig:partition-taxonomy}. Then, we systematically explore and find the optimized scheme.

\subsubsection{Taxonomy of Partitioning Schemes}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/partition_taxonomy.pdf}
\caption{Taxonomy of layer partitioning schemes across PEs. (a) Batch partitioning divides different images across PEs. (b) Fmap partitioning divides spatial regions. (c) Output partitioning divides output channels. (d) Input partitioning divides input channels and requires reduction.}
\label{fig:partition-taxonomy}
\end{figure}

\textbf{Batch Partitioning (Data-parallelism):} The simplest scheme is to use multiple PEs to process multiple input images in parallel, effectively dividing a batch of images across PEs~\cite{data-parallel}. While good for throughput, it requires the NN model to be replicated in each PE, which is a significant capacity challenge for large NNs. Moreover, parallelism is limited by the batch size. This scheme is less attractive for latency-sensitive, real-time applications since it does not improve the latency for inference on each image.

\textbf{Fmap (Image) Partitioning:} If the feature map is large (e.g., $112 \times 112$), we can partition it into smaller tiles as shown in Figure~\ref{fig:partition-taxonomy}(b). The smaller fmap tile will fit better into the PE array and reduce the need for folding in the row stationary dataflow~\cite{eyeriss}. Moreover, if the ifmaps and ofmaps in a CONV layer use the same fmap partitioning, most data accesses will be local within each PE due to the locality of 2D convolution. However, the filters need to be replicated across all PEs.

\textbf{Output Partitioning:} As each layer usually has multiple output feature maps (ofmaps), we can partition the ofmaps across PEs. For example, we can divide the $K$ ofmaps into $p$ groups and each PE processes $K/p$ ofmaps. Since each ofmap uses different filters, the filter weights can be fully partitioned. Since all ifmaps contribute to all ofmaps, all ifmaps must be sent to all PEs, which requires remote PE accesses.

\textbf{Input Partitioning:} Similar to output partitioning, we could also partition the $C$ ifmaps across PEs. The difference is where the computations take place. However, access to ofmaps generates both read and write traffic, and thus is more critical than the ifmap access. Hence, it is better to use output partitioning that avoids the remote accesses for ofmaps, rather than using input partitioning that avoids the remote accesses for ifmaps. \textbf{Importantly, input partitioning requires an All-Reduce operation to aggregate partial sums across PEs}, which introduces significant communication overhead.

Table~\ref{tab:partition-comparison} summarizes the characteristics of each partitioning scheme.

\begin{table}[htbp]
\centering
\caption{Comparison of partitioning schemes}
\label{tab:partition-comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Scheme} & \textbf{Weight} & \textbf{IFmap} & \textbf{OFmap} & \textbf{Reduction} \\
\midrule
Batch (BATP) & Replicate & Partition & Partition & No \\
Fmap (OFMP) & Replicate & Partition & Partition & No \\
Output (OUTP) & Partition & Broadcast & Partition & No \\
Input (INPP) & Partition & Partition & \textbf{All-Reduce} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hybrid Partitioning Scheme}

Previous work~\cite{neurocube} used a simple heuristic to minimize remote PE accesses, i.e., fmap partitioning for CONV layers and output partitioning for FC layers. This heuristic does not necessarily lead to the best performance and energy. In addition to the number of remote accesses, we should consider the impact of the partitioning scheme on the total number of memory accesses and the data reuse in the PE register files.

Because the fmaps are tiled with fmap partitioning, each PE needs to load the same filters into the PE registers, while only reusing them across the smaller fmap tile. In contrast, output partitioning keeps the whole fmap in one PE, loads the filters once, and uses them across the whole fmap which results in higher filter reuse.

Therefore, we consider a \textbf{hybrid partitioning scheme} that strikes a balance between the benefits of fmap partitioning (minimizing remote accesses) and those of output partitioning (better on-chip data reuse to minimize total DRAM accesses). Such a hybrid scheme also captures the intention of the Neurocube heuristic:
\begin{itemize}
    \item For the first few CONV layers in common CNNs, the access to large fmaps dominates and the weight reuse is not significant, thus we should mostly use \textbf{fmap partitioning}.
    \item For the FC layers, the filter weights are much larger than the fmaps, so \textbf{output partitioning} is preferred to maximize weight reuse.
\end{itemize}

\subsubsection{Cost Model}

To find the optimized partitioning scheme, we propose a cost model in terms of overall memory access energy:
\begin{equation}
E_{\text{access}} = A_{\text{DRAM}} \times e \times (1 + \beta \cdot r)
\label{eq:access-cost}
\end{equation}
where $e$ is the energy for one DRAM access to the local PE, $\beta$ is the energy penalty for one remote PE access, and $r$ is the percentage of accesses to remote PEs. $A_{\text{DRAM}}$ is the total number of main memory accesses. Fmap partitioning minimizes $r$ but results in larger $A_{\text{DRAM}}$, while output partitioning has smaller $A_{\text{DRAM}}$ by sacrificing $r$.

\textbf{INPP All-Reduce Cost:} When using input channel partitioning (INPP), each PE computes partial sums that must be aggregated through an All-Reduce operation:
\begin{equation}
E_{\text{INPP}} = D_{\text{output}} \times \frac{2(p-1)}{p} \times e_{\text{comm}}
\label{eq:inpp-cost}
\end{equation}
where $D_{\text{output}}$ is the output data size, $p$ is the INPP partition factor, and $e_{\text{comm}}$ is the per-byte communication energy. The factor $\frac{2(p-1)}{p}$ comes from the Ring All-Reduce algorithm (Reduce-Scatter followed by All-Gather).

\subsubsection{Layer-Type Specific Constraints}

Different layer types have different valid partition dimensions, as summarized in Table~\ref{tab:layer-constraints}.

\begin{table}[htbp]
\centering
\caption{Valid partition dimensions for different layer types}
\label{tab:layer-constraints}
\begin{tabular}{lccccl}
\toprule
\textbf{Layer Type} & \textbf{OUTP} & \textbf{OFMP} & \textbf{BATP} & \textbf{INPP} & \textbf{Notes} \\
\midrule
Conv & \checkmark & \checkmark & \checkmark & \checkmark & All dimensions valid \\
FC & \checkmark & $\times$ & \checkmark & \checkmark & OFMP invalid ($H{=}W{=}1$) \\
Pool & \checkmark & \checkmark & \checkmark & $\times$ & INPP invalid (no cross-channel) \\
Eltwise & \checkmark & \checkmark & \checkmark & $\times$ & No reduction, pass-through \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Cross-Layer Propagation}

Each NN layer can be potentially parallelized using a combination of fmap partitioning and output partitioning. \textbf{Adjacent layers are coupled because the partitioning scheme used for the previous layer determines the layout of ifmaps of the next layer} (see Figure~\ref{fig:partition-taxonomy}). 

The partition scheme propagation across layers includes:
\begin{enumerate}
    \item \textbf{OUTP (K) $\rightarrow$ C:} Layer $l$'s output channel partition becomes Layer $l{+}1$'s input channel distribution. If Layer $l$ uses OUTP=$p$ and Layer $l{+}1$ uses INPP=$p$, the distribution matches and no redistribution is needed.
    \item \textbf{OFMP (H,W) $\rightarrow$ Spatial:} Spatial partitioning propagates to the next layer's input spatial distribution, considering halo regions for convolution boundaries.
    \item \textbf{BATP (N) $\rightarrow$ Batch:} Batch partitioning typically matches across layers with no redistribution needed.
    \item \textbf{INPP (C) $\rightarrow$ Reduction:} Input partitioning requires All-Reduce within the layer and does not affect the next layer's layout.
\end{enumerate}

\subsubsection{Partitioning Exploration}

Assuming $L$ layers and $C$ partitioning combinations per layer, we must consider the overall memory access energy cost for $C^L$ scenarios. As $L$ can be 20 to 100 and $C$ can be 4 to 8, the cost is prohibitive.

We leverage three strategies to reduce the difficulty of partitioning exploration:

\textbf{Strategy 1: Greedy Algorithm.} We use a greedy algorithm that explores the partitioning options for layer $i$ without backtracing, assuming the partitioning of the first $i-1$ layers in the optimal scheme is independent of that for the later layers. The first CONV layer is assumed to only use fmap partitioning. This allows us to reduce the number of choices to $C \times L$, roughly several thousands in common cases.

\textbf{Strategy 2: Layout Propagation.} Based on reduction analysis, we can reduce partition decision variables:
\begin{itemize}
    \item \textbf{Operators with reduction} (Conv, FC, Pool) $\rightarrow$ Layout-sensitive $\rightarrow$ Independent partition decision required
    \item \textbf{Operators without reduction} (Eltwise, ReLU, Add) $\rightarrow$ Layout-insensitive $\rightarrow$ Can propagate upstream partition
\end{itemize}

This allows us to reduce decision variables from $L$ layers to $G$ groups ($G \leq L$), where layers in the same group share the same partition:
\begin{equation}
\text{Original variables: } L \times C \quad \Rightarrow \quad \text{Optimized: } G \times C
\end{equation}

For example, in ResNet-50, $G/L \approx 78\%$, reducing 22\% of decision variables. In ResNet-152, the reduction reaches 24\%.

\textbf{Strategy 3: Dynamic Programming / ILP.} For moderate-scale problems, we use dynamic programming or integer linear programming (ILP) to find the global optimum:
\begin{equation}
\min \sum_{l} E_{\text{compute}}(l) + \sum_{l} E_{\text{redistribute}}(l, l{+}1)
\end{equation}
subject to partition factor constraints, total node constraints, and layer-type constraints.

The DP approach has complexity $O(L \times C^2)$, which is tractable for typical networks.
