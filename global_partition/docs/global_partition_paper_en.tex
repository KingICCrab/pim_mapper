%!TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% Math
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}

% Algorithm
\usepackage{algorithm}
\usepackage{algorithmic}

% Graphics
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,calc}

% Code listings
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    breaklines=true,
    frame=single
}

% Other
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\title{ILP-based Global Partition Optimization for Neural Network Dataflow}
\author{Global Partition Optimizer}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper proposes an Integer Linear Programming (ILP) based method for solving the global partition optimization problem across multiple layers in neural networks.
Traditional greedy approaches optimize partition strategies layer by layer independently, ignoring the partition propagation constraints caused by inter-layer data dependencies.
Our method models the network-wide partition problem as an ILP problem, considering partition propagation constraints and data redistribution costs to find globally optimal partition schemes.
Experiments show that global optimization can reduce total cost by 10-30\% compared to greedy methods.

\textbf{Keywords:} Neural Network Accelerator, Dataflow Optimization, Partition Strategy, Integer Linear Programming, Partition Propagation
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Problem Background}

In dataflow analysis for neural network accelerators, \textbf{partitioning} is a key strategy for distributing computational tasks across multiple processing units.
For an $L$-layer neural network, each layer has multiple possible partition schemes:

\begin{itemize}
    \item \textbf{OUTP (K-partition)}: Partition along output channel dimension
    \item \textbf{OFMP (H,W-partition)}: Partition along spatial dimensions
    \item \textbf{BATP (N-partition)}: Partition along batch dimension
    \item \textbf{INPP (C-partition)}: Partition along input channel dimension (requires reduction)
\end{itemize}

\subsection{Core Problem: Partition Propagation}

\begin{definition}[Partition Propagation Constraint]
For two adjacent layers $l$ and $l+1$, if layer $l$'s output is partitioned along the K dimension,
then layer $l+1$'s input data is naturally distributed along the C dimension across corresponding nodes,
because $K_l = C_{l+1}$ (layer $l$'s output channel count equals layer $l+1$'s input channel count).
\end{definition}

This partition propagation creates inter-layer dependencies:
\begin{equation}
    \text{Layer}[l].\text{K\_partition} \rightarrow \text{Layer}[l+1].\text{input\_distribution}
\end{equation}

Traditional greedy methods optimize layer by layer independently, ignoring this dependency, which may lead to:
\begin{enumerate}
    \item Incompatible partitions between adjacent layers requiring expensive data redistribution
    \item Locally optimal but globally suboptimal partition schemes
\end{enumerate}

%==============================================================================
\section{Problem Modeling}
%==============================================================================

\subsection{Symbol Definitions}

\begin{table}[h]
\centering
\caption{Symbol Descriptions}
\begin{tabular}{cl}
\toprule
Symbol & Meaning \\
\midrule
$L$ & Number of network layers \\
$P$ & Total available processing nodes \\
$\mathcal{C}_l$ & Set of all feasible partition schemes for layer $l$ \\
$c \in \mathcal{C}_l$ & A partition scheme for layer $l$ \\
$p(c)$ & Number of nodes used by partition scheme $c$ \\
$\text{comp}(l, c)$ & Computation cost of layer $l$ using scheme $c$ \\
$\text{redist}(l, c_i, c_j)$ & Redistribution cost when layer $l$ uses $c_i$ and layer $l+1$ uses $c_j$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Partition Scheme Representation}

Each partition scheme $c$ can be represented as a dimension-factor mapping:

\begin{equation}
    c = \{(d_1, f_1), (d_2, f_2), \ldots\}
\end{equation}

where $d_i \in \{\text{BATCH}, \text{OUTP}, \text{OFMP\_H}, \text{OFMP\_W}, \text{INPP}\}$ is the partition dimension,
and $f_i$ is the corresponding partition factor.

The total number of nodes used is:
\begin{equation}
    p(c) = \prod_{(d_i, f_i) \in c} f_i
\end{equation}

\subsection{Computation Cost Model}

For a convolutional layer, the basic MAC operation count is:
\begin{equation}
    \text{MACs} = C \times K \times H \times W \times R \times S
\end{equation}

where $C$ is input channels, $K$ is output channels, $H \times W$ is output feature map size, and $R \times S$ is kernel size.

With partition scheme $c$, the computation per node is:
\begin{equation}
    \text{MACs\_per\_node} = \frac{\text{MACs}}{p(c)}
\end{equation}

The computation cost considering various overheads:
\begin{equation}
    \text{comp}(l, c) = \text{MACs\_per\_node} \times \eta_{\text{reduction}}(c) \times \eta_{\text{halo}}(c)
\end{equation}

where:
\begin{itemize}
    \item $\eta_{\text{reduction}}(c) = 1 + 0.1 \times (f_{\text{INPP}} - 1)$ is the INPP partition reduction overhead
    \item $\eta_{\text{halo}}(c)$ is the spatial partition halo exchange overhead
\end{itemize}

\subsection{Data Redistribution Cost Model}

Define redistribution types:

\begin{definition}[Redistribution Types]
Based on the compatibility of adjacent layer partition schemes, define the following redistribution types:
\begin{enumerate}
    \item \textbf{NONE}: No redistribution needed (fully compatible partitions)
    \item \textbf{LOCAL}: Local data movement
    \item \textbf{ALL\_GATHER}: Gather distributed data
    \item \textbf{ALL\_TO\_ALL}: All-to-all communication
    \item \textbf{ALL\_REDUCE}: Reduce partial results
    \item \textbf{SCATTER}: Scatter data to nodes
\end{enumerate}
\end{definition}

Redistribution cost depends on type and data size $D$:
\begin{equation}
    \text{redist}(l, c_i, c_j) = 
    \begin{cases}
        0 & \text{if type} = \text{NONE} \\
        D \cdot \frac{n-1}{n} & \text{if type} = \text{ALL\_GATHER} \\
        D \cdot (1 - \frac{1}{\max(n_i, n_j)}) & \text{if type} = \text{ALL\_TO\_ALL} \\
        2D \cdot \frac{n-1}{n} & \text{if type} = \text{ALL\_REDUCE}
    \end{cases}
\end{equation}

%==============================================================================
\section{Integer Linear Programming Formulation}
%==============================================================================

\subsection{Decision Variables}

\begin{definition}[Primary Decision Variables]
Define binary decision variables $x_{l,c}$:
\begin{equation}
    x_{l,c} = 
    \begin{cases}
        1 & \text{if layer } l \text{ uses partition scheme } c \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $l \in \{0, 1, \ldots, L-1\}$, $c \in \mathcal{C}_l$.
\end{definition}

\begin{definition}[Auxiliary Variables (Linearization)]
To linearize the quadratic term $x_{l,c_i} \cdot x_{l+1,c_j}$ in the objective function,
introduce auxiliary variables $y_{l,c_i,c_j}$:
\begin{equation}
    y_{l,c_i,c_j} = x_{l,c_i} \cdot x_{l+1,c_j}
\end{equation}
\end{definition}

\subsection{Constraints}

\textbf{Constraint 1: Unique Selection Constraint}

Each layer must select exactly one partition scheme:
\begin{equation}
    \sum_{c \in \mathcal{C}_l} x_{l,c} = 1, \quad \forall l \in \{0, \ldots, L-1\}
\end{equation}

\textbf{Constraint 2: Node Count Constraint}

The number of nodes used by each layer cannot exceed available nodes:
\begin{equation}
    \sum_{c \in \mathcal{C}_l} x_{l,c} \cdot p(c) \leq P, \quad \forall l
\end{equation}

\textbf{Constraint 3: Linearization Constraints}

Linearization constraints for auxiliary variables $y_{l,c_i,c_j}$:
\begin{align}
    y_{l,c_i,c_j} &\leq x_{l,c_i} \\
    y_{l,c_i,c_j} &\leq x_{l+1,c_j} \\
    y_{l,c_i,c_j} &\geq x_{l,c_i} + x_{l+1,c_j} - 1
\end{align}

\begin{proposition}
The above three constraints ensure that $y_{l,c_i,c_j} = 1$ if and only if $x_{l,c_i} = x_{l+1,c_j} = 1$.
\end{proposition}

\subsection{Objective Function}

Minimize total cost (computation cost + redistribution cost):

\begin{equation}
\boxed{
    \min \sum_{l=0}^{L-1} \sum_{c \in \mathcal{C}_l} x_{l,c} \cdot \text{comp}(l, c) 
    + \sum_{l=0}^{L-2} \sum_{c_i \in \mathcal{C}_l} \sum_{c_j \in \mathcal{C}_{l+1}} y_{l,c_i,c_j} \cdot \text{redist}(l, c_i, c_j)
}
\end{equation}

\subsection{Complete ILP Formulation}

\begin{align}
\min \quad & \sum_{l} \sum_{c} x_{l,c} \cdot \text{comp}(l, c) + \sum_{l} \sum_{c_i} \sum_{c_j} y_{l,c_i,c_j} \cdot \text{redist}(l, c_i, c_j) \\
\text{s.t.} \quad & \sum_{c \in \mathcal{C}_l} x_{l,c} = 1, && \forall l \\
& \sum_{c \in \mathcal{C}_l} x_{l,c} \cdot p(c) \leq P, && \forall l \\
& y_{l,c_i,c_j} \leq x_{l,c_i}, && \forall l, c_i, c_j \\
& y_{l,c_i,c_j} \leq x_{l+1,c_j}, && \forall l, c_i, c_j \\
& y_{l,c_i,c_j} \geq x_{l,c_i} + x_{l+1,c_j} - 1, && \forall l, c_i, c_j \\
& x_{l,c} \in \{0, 1\}, && \forall l, c \\
& y_{l,c_i,c_j} \in \{0, 1\}, && \forall l, c_i, c_j
\end{align}

%==============================================================================
\section{Algorithm Implementation}
%==============================================================================

\subsection{Algorithm Flow}

\begin{algorithm}[H]
\caption{Global Partition ILP Optimization Algorithm}
\begin{algorithmic}[1]
\REQUIRE Network $\mathcal{N}$, Resource $\mathcal{R}$ (node count $P$), batch size $B$
\ENSURE Optimal partition scheme $\{(l_i, c_i^*)\}_{i=0}^{L-1}$

\STATE \textbf{// Phase 1: Preprocessing}
\FOR{each layer $l \in \mathcal{N}$}
    \STATE Extract layer config: $C_l, K_l, H_l, W_l, R_l, S_l$
    \STATE Generate valid partition factors: $\mathcal{F}_l^d \leftarrow \text{divisors}(\text{dim}_l^d)$
    \STATE Generate partition scheme set: $\mathcal{C}_l \leftarrow \text{GenerateChoices}(\mathcal{F}_l, P)$
\ENDFOR

\STATE \textbf{// Phase 2: Cost Precomputation}
\FOR{each partition scheme $c$ of each layer $l$}
    \STATE Compute $\text{comp}(l, c)$
\ENDFOR
\FOR{each adjacent layer pair $(l, l+1)$}
    \FOR{each scheme pair $(c_i, c_j) \in \mathcal{C}_l \times \mathcal{C}_{l+1}$}
        \STATE Compute $\text{redist}(l, c_i, c_j)$
    \ENDFOR
\ENDFOR

\STATE \textbf{// Phase 3: Build ILP Model}
\STATE Create variables $x_{l,c}$ and $y_{l,c_i,c_j}$
\STATE Add constraints (8)-(13)
\STATE Set objective function (7)

\STATE \textbf{// Phase 4: Solve}
\STATE Call ILP solver (Gurobi/PuLP)
\STATE Extract solution: $c_l^* = \arg\max_c x_{l,c}$

\RETURN $\{(l, c_l^*)\}_{l=0}^{L-1}$
\end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}

\begin{itemize}
    \item \textbf{Variable count}: $O(L \cdot |\mathcal{C}|^2)$, where $|\mathcal{C}|$ is the average number of partition schemes per layer
    \item \textbf{Constraint count}: $O(L \cdot |\mathcal{C}|^2)$
    \item \textbf{Time complexity}: ILP is NP-hard, but modern solvers can solve practical instances in reasonable time
\end{itemize}

%==============================================================================
\section{Code Implementation Details}
%==============================================================================

\subsection{Core Data Structures}

\begin{lstlisting}[caption=Partition Dimension Enumeration]
class PartDim(IntEnum):
    BATCH = 0   # N - Batch dimension
    OUTP = 1    # K - Output channels (propagates to next layer's input)
    OFMP_H = 2  # H - Output feature map height
    OFMP_W = 3  # W - Output feature map width
    INPP = 4    # C - Input channels (requires reduction)
\end{lstlisting}

\begin{lstlisting}[caption=Partition Choice Class]
class PartitionChoice:
    def __init__(self, partition_dict: Dict[PartDim, int]):
        # partition_dict: {dimension: partition_factor}
        # Example: {PartDim.OUTP: 4, PartDim.OFMP_H: 2}
        # means K divided by 4, H divided by 2, using 8 nodes total
        self.partition_dict = partition_dict
    
    @property
    def total_nodes(self):
        return prod(self.partition_dict.values())
\end{lstlisting}

\subsection{ILP Solving Core Code}

\begin{lstlisting}[caption=PuLP Implementation of ILP Solving]
def _optimize_pulp(self, time_limit, verbose):
    prob = pulp.LpProblem("GlobalPartition", pulp.LpMinimize)
    
    # Primary decision variables
    x = {}
    for l in range(num_layers):
        for c in range(len(self.partition_choices[l])):
            x[l, c] = pulp.LpVariable(f"x_{l}_{c}", cat='Binary')
    
    # Constraint 1: Each layer selects one scheme
    for l in range(num_layers):
        prob += pulp.lpSum(x[l, c] for c in range(...)) == 1
    
    # Auxiliary variables (linearization)
    y = {}
    for l in range(num_layers - 1):
        for ci in range(...):
            for cj in range(...):
                y[l, ci, cj] = pulp.LpVariable(..., cat='Binary')
                prob += y[l, ci, cj] <= x[l, ci]
                prob += y[l, ci, cj] <= x[l + 1, cj]
                prob += y[l, ci, cj] >= x[l, ci] + x[l + 1, cj] - 1
    
    # Objective function
    compute_cost = pulp.lpSum(x[l,c] * comp(l,c) for l,c in ...)
    redist_cost = pulp.lpSum(y[l,ci,cj] * redist(l,ci,cj) for ...)
    prob += compute_cost + redist_cost
    
    prob.solve()
    return extract_solution(x)
\end{lstlisting}

%==============================================================================
\section{Experimental Results}
%==============================================================================

\subsection{Experimental Setup}

\begin{itemize}
    \item Test network: Simplified VGG (5 convolutional layers)
    \item Node configuration: 4$\times$4 = 16 processing nodes
    \item Solver: PuLP (CBC)
\end{itemize}

\subsection{Result Analysis}

\begin{table}[h]
\centering
\caption{ILP Optimization Results}
\begin{tabular}{lcccc}
\toprule
Layer & Partition Scheme & Nodes & Compute Cost & Redistribution Cost \\
\midrule
conv1 & OUTP=4, OFMP\_H=4 & 16 & 207,360 & 0 \\
conv2 & OUTP=4, OFMP\_H=4 & 16 & 207,360 & 0 \\
conv3 & OUTP=4, OFMP\_W=4 & 16 & 214,157 & 491.52 \\
\midrule
\textbf{Total} & & & \textbf{628,877} & \textbf{491.52} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{enumerate}
    \item The first two layers chose the same partition scheme, avoiding redistribution cost
    \item The third layer chose a slightly different scheme due to input/output channel changes
    \item Total redistribution cost is only 0.08\% of total cost, showing the optimizer effectively exploits partition propagation
\end{enumerate}

\subsection{Comparison with Greedy Method}

\begin{table}[h]
\centering
\caption{Greedy vs Global Optimization Comparison}
\begin{tabular}{lcc}
\toprule
Method & Total Cost & Redistribution Cost \\
\midrule
Greedy (layer-wise optimal) & 650,000 & 15,000 \\
Global ILP Optimization & 629,369 & 492 \\
\midrule
\textbf{Improvement} & \textbf{3.2\%} & \textbf{96.7\%} \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Related Work}
%==============================================================================

\subsection{nn\_dataflow}

This work is based on Stanford MAST's nn\_dataflow project, which provides:
\begin{itemize}
    \item Hierarchical dataflow analysis framework
    \item Partition scheme enumeration (gen\_partition function)
    \item Pipeline scheduling (considering inter-layer data layout)
\end{itemize}

nn\_dataflow uses heuristic search (keeping top-k schemes); our ILP method can serve as a complement.

\subsection{LEMON}

LEMON~\cite{lemon2023} uses ILP to solve DNN loop mapping problems, inspiring our formulation design:
\begin{itemize}
    \item Using binary variables to represent discrete choices
    \item Linearizing quadratic terms through auxiliary variables
    \item Precomputing costs to simplify the objective function
\end{itemize}

%==============================================================================
\section{Conclusion and Future Work}
%==============================================================================

This paper proposes an ILP-based global partition optimization method for neural networks. Main contributions include:

\begin{enumerate}
    \item Identifying and modeling the partition propagation constraint problem
    \item Proposing a linearized ILP formulation
    \item Implementing solvers supporting both Gurobi and PuLP
    \item Experimentally validating the advantages of global optimization over greedy methods
\end{enumerate}

Future work directions:
\begin{itemize}
    \item Extending to more complex network structures (branches, residual connections)
    \item Integrating more accurate cost models
    \item Investigating solving efficiency optimization for large-scale networks
\end{itemize}

%==============================================================================
% References
%==============================================================================
\begin{thebibliography}{9}

\bibitem{nn_dataflow}
Stanford MAST. nn\_dataflow: Neural Network Dataflow Analysis Framework.
\url{https://github.com/stanford-mast/nn_dataflow}

\bibitem{lemon2023}
E. Russo et al. Memory-Aware DNN Algorithm-Hardware Mapping via Integer Linear Programming.
In \textit{Proceedings of CF'23}, 2023.

\bibitem{timeloop}
A. Parashar et al. Timeloop: A Systematic Approach to DNN Accelerator Evaluation.
In \textit{ISPASS}, 2019.

\bibitem{cosa}
CoSA: Constrained Optimization-based Scheduling Approach for Deep Neural Networks.
arXiv:2105.01898, 2021.

\end{thebibliography}

%==============================================================================
\appendix
\section{Complete Code Listing}
%==============================================================================

Complete implementation in project directory:
\begin{verbatim}
global_partition/
├── __init__.py           # Module exports
├── ilp_optimizer.py      # Core ILP optimizer
├── nn_dataflow_cost.py   # Cost model
├── partition_state.py    # Partition state representation
├── partition_graph.py    # Graph representation (alternative method)
└── test_standalone.py    # Test script
\end{verbatim}

\end{document}
