#!/usr/bin/env python
"""
Layout Propagation æ¨¡å—

æ ¸å¿ƒåŸåˆ™ï¼š
- æœ‰è§„çº¦è®¡ç®—çš„ç®—å­ â†’ å¸ƒå±€æ•æ„Ÿï¼ˆLayout Sensitiveï¼‰
- æ— è§„çº¦çš„ç®—å­ï¼ˆElementwiseï¼‰â†’ å¸ƒå±€ä¸æ•æ„Ÿï¼Œå¯ä»¥é€ä¼ åˆ†åŒº

è§„çº¦åˆ†æï¼š
- Conv: åœ¨ C (è¾“å…¥é€šé“) ç»´åº¦è§„çº¦ â†’ æ•æ„Ÿ
- FC/MatMul: åœ¨ K (å†…ç§¯) ç»´åº¦è§„çº¦ â†’ æ•æ„Ÿ  
- Pool: åœ¨ç©ºé—´çª—å£è§„çº¦ â†’ æ•æ„Ÿ
- Softmax: åœ¨æŸä¸ªç»´åº¦è§„çº¦ â†’ æ•æ„Ÿ
- BatchNorm: åœ¨ N (batch) ç»´åº¦è§„çº¦ç»Ÿè®¡é‡ â†’ æ•æ„Ÿï¼ˆè®­ç»ƒæ—¶ï¼‰/ ä¸æ•æ„Ÿï¼ˆæ¨ç†æ—¶ï¼‰
- ReLU/Add/Mul: é€å…ƒç´ æ“ä½œï¼Œæ— è§„çº¦ â†’ ä¸æ•æ„Ÿ
"""

from collections import deque
from typing import List, Dict, Set, Tuple, Optional, Any
from enum import Enum
from dataclasses import dataclass


class LayoutSensitivity(Enum):
    """ç®—å­çš„å¸ƒå±€æ•æ„Ÿæ€§"""
    SENSITIVE = "sensitive"      # æœ‰è§„çº¦ï¼Œå¸ƒå±€æ•æ„Ÿ
    INSENSITIVE = "insensitive"  # æ— è§„çº¦ï¼Œå¯ä»¥é€ä¼ 


@dataclass
class OperatorInfo:
    """ç®—å­ä¿¡æ¯"""
    name: str
    op_type: str
    has_reduction: bool  # æ˜¯å¦æœ‰è§„çº¦è®¡ç®—
    reduction_dims: List[str]  # åœ¨å“ªäº›ç»´åº¦ä¸Šè§„çº¦
    input_shape: Tuple[int, ...]
    output_shape: Tuple[int, ...]

    @property
    def sensitivity(self) -> LayoutSensitivity:
        """æ ¹æ®æ˜¯å¦æœ‰è§„çº¦åˆ¤æ–­å¸ƒå±€æ•æ„Ÿæ€§"""
        if self.has_reduction:
            return LayoutSensitivity.SENSITIVE
        return LayoutSensitivity.INSENSITIVE


class ReductionAnalyzer:
    """
    è§„çº¦åˆ†æå™¨

    åˆ†ææ¯ä¸ªç®—å­æ˜¯å¦åŒ…å«è§„çº¦æ“ä½œï¼Œä»¥æ­¤åˆ¤æ–­å¸ƒå±€æ•æ„Ÿæ€§ã€‚
    """

    # å·²çŸ¥ç®—å­ç±»å‹çš„è§„çº¦ç‰¹æ€§
    REDUCTION_PATTERNS = {
        # æœ‰è§„çº¦çš„ç®—å­
        'Conv': {'has_reduction': True, 'reduction_dims': ['C', 'R', 'S']},
        'ConvLayer': {'has_reduction': True, 'reduction_dims': ['C', 'R', 'S']},
        'FC': {'has_reduction': True, 'reduction_dims': ['C']},
        'FCLayer': {'has_reduction': True, 'reduction_dims': ['C']},
        'MatMul': {'has_reduction': True, 'reduction_dims': ['K']},
        'Gemm': {'has_reduction': True, 'reduction_dims': ['K']},
        'Pool': {'has_reduction': True, 'reduction_dims': ['H', 'W']},
        'PoolingLayer': {'has_reduction': True, 'reduction_dims': ['H', 'W']},
        'MaxPool': {'has_reduction': True, 'reduction_dims': ['H', 'W']},
        'AvgPool': {'has_reduction': True, 'reduction_dims': ['H', 'W']},
        'GlobalAvgPool': {'has_reduction': True, 'reduction_dims': ['H', 'W']},
        'Softmax': {'has_reduction': True, 'reduction_dims': ['C']},
        'ReduceSum': {'has_reduction': True, 'reduction_dims': ['axis']},
        'ReduceMean': {'has_reduction': True, 'reduction_dims': ['axis']},
        'ReduceMax': {'has_reduction': True, 'reduction_dims': ['axis']},

        # æ— è§„çº¦çš„ç®—å­ï¼ˆElementwiseï¼‰
        'ReLU': {'has_reduction': False, 'reduction_dims': []},
        'Sigmoid': {'has_reduction': False, 'reduction_dims': []},
        'Tanh': {'has_reduction': False, 'reduction_dims': []},
        'LeakyReLU': {'has_reduction': False, 'reduction_dims': []},
        'Add': {'has_reduction': False, 'reduction_dims': []},
        'Sub': {'has_reduction': False, 'reduction_dims': []},
        'Mul': {'has_reduction': False, 'reduction_dims': []},
        'Div': {'has_reduction': False, 'reduction_dims': []},
        'Concat': {'has_reduction': False, 'reduction_dims': []},
        'Split': {'has_reduction': False, 'reduction_dims': []},
        'Reshape': {'has_reduction': False, 'reduction_dims': []},
        'Transpose': {'has_reduction': False, 'reduction_dims': []},
        'Eltwise': {'has_reduction': False, 'reduction_dims': []},
        'EltwiseLayer': {'has_reduction': False, 'reduction_dims': []},

        # BatchNorm: æ¨ç†æ—¶æ— è§„çº¦ï¼Œè®­ç»ƒæ—¶æœ‰è§„çº¦
        'BatchNorm': {'has_reduction': False, 'reduction_dims': []},  # é»˜è®¤æ¨ç†
        'BatchNormalization': {'has_reduction': False, 'reduction_dims': []},

        # LocalRegion: æœ‰å±€éƒ¨è§„çº¦
        'LocalRegion': {'has_reduction': True, 'reduction_dims': ['local']},
        'LocalRegionLayer': {'has_reduction': True, 'reduction_dims': ['local']},
        'LRN': {'has_reduction': True, 'reduction_dims': ['local']},
    }

    @classmethod
    def analyze(cls, layer) -> OperatorInfo:
        """
        åˆ†æå•ä¸ªç®—å­çš„è§„çº¦ç‰¹æ€§ã€‚

        Args:
            layer: ç®—å­å¯¹è±¡ï¼ˆnn_dataflow Layer æˆ–ç±»ä¼¼å¯¹è±¡ï¼‰

        Returns:
            OperatorInfo åŒ…å«è§„çº¦åˆ†æç»“æœ
        """
        # è·å–ç®—å­ç±»å‹
        class_name = layer.__class__.__name__
        op_type = cls._get_op_type(class_name)

        # è·å–å½¢çŠ¶ä¿¡æ¯
        input_shape = cls._get_input_shape(layer)
        output_shape = cls._get_output_shape(layer)

        # æŸ¥æ‰¾è§„çº¦æ¨¡å¼
        pattern = cls.REDUCTION_PATTERNS.get(op_type, None)

        if pattern is None:
            # æœªçŸ¥ç®—å­ï¼Œå°è¯•å¯å‘å¼åˆ¤æ–­
            has_reduction, reduction_dims = cls._heuristic_analysis(layer)
        else:
            has_reduction = pattern['has_reduction']
            reduction_dims = pattern['reduction_dims']

        return OperatorInfo(
            name=getattr(layer, 'name', class_name),
            op_type=op_type,
            has_reduction=has_reduction,
            reduction_dims=reduction_dims,
            input_shape=input_shape,
            output_shape=output_shape
        )

    @classmethod
    def _get_op_type(cls, class_name: str) -> str:
        """ä»ç±»åæå–ç®—å­ç±»å‹"""
        # ç§»é™¤å¸¸è§åç¼€
        for suffix in ['Layer', 'Op', 'Operation']:
            if class_name.endswith(suffix) and class_name != suffix:
                return class_name
        return class_name

    @classmethod
    def _get_input_shape(cls, layer) -> Tuple[int, ...]:
        """è·å–è¾“å…¥å½¢çŠ¶"""
        if hasattr(layer, 'nifm') and hasattr(layer, 'hifm'):
            return (getattr(layer, 'nifm', 1),
                    getattr(layer, 'hifm', 1),
                    getattr(layer, 'wifm', 1))
        return ()

    @classmethod
    def _get_output_shape(cls, layer) -> Tuple[int, ...]:
        """è·å–è¾“å‡ºå½¢çŠ¶"""
        if hasattr(layer, 'nofm') and hasattr(layer, 'hofm'):
            return (getattr(layer, 'nofm', 1),
                    getattr(layer, 'hofm', 1),
                    getattr(layer, 'wofm', 1))
        return ()

    @classmethod
    def _heuristic_analysis(cls, layer) -> Tuple[bool, List[str]]:
        """
        å¯å‘å¼åˆ†ææœªçŸ¥ç®—å­ã€‚

        è§„åˆ™ï¼š
        1. å¦‚æœè¾“å‡ºå…ƒç´ æ•° < è¾“å…¥å…ƒç´ æ•° â†’ å¯èƒ½æœ‰è§„çº¦
        2. å¦‚æœæœ‰ filter/kernel å±æ€§ â†’ å¯èƒ½æœ‰è§„çº¦
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ filterï¼ˆå·ç§¯ç±»ï¼‰
        if hasattr(layer, 'hfil') and getattr(layer, 'hfil', 1) > 1:
            return True, ['C', 'R', 'S']

        # æ£€æŸ¥è¾“å…¥è¾“å‡ºå¤§å°
        input_shape = cls._get_input_shape(layer)
        output_shape = cls._get_output_shape(layer)

        if input_shape and output_shape:
            input_size = 1
            output_size = 1
            for s in input_shape:
                input_size *= s
            for s in output_shape:
                output_size *= s

            # è¾“å‡ºæ¯”è¾“å…¥å°å¾ˆå¤š â†’ å¯èƒ½æœ‰è§„çº¦
            if output_size < input_size * 0.5:
                return True, ['unknown']

        # é»˜è®¤æ— è§„çº¦
        return False, []


class LayoutPropagator:
    """
    å¸ƒå±€/åˆ†åŒºä¼ æ’­å™¨

    åŸºäºè§„çº¦åˆ†æçš„å¸ƒå±€æ•æ„Ÿæ€§ï¼Œåœ¨è®¡ç®—å›¾ä¸­ä¼ æ’­åˆ†åŒºæ–¹æ¡ˆã€‚

    è§„åˆ™ï¼š
    - å¸ƒå±€æ•æ„Ÿç®—å­ï¼ˆæœ‰è§„çº¦ï¼‰ï¼šæ˜¯ä¼ æ’­çš„èµ·ç‚¹å’Œç»ˆç‚¹
    - å¸ƒå±€ä¸æ•æ„Ÿç®—å­ï¼ˆæ— è§„çº¦ï¼‰ï¼šå¯ä»¥é€ä¼ ä¸Šæ¸¸æˆ–ä¸‹æ¸¸çš„åˆ†åŒº
    """

    def __init__(self, operators: List[OperatorInfo],
                 adjacency: Dict[int, List[int]] = None):
        """
        Args:
            operators: ç®—å­ä¿¡æ¯åˆ—è¡¨
            adjacency: é‚»æ¥è¡¨ {op_idx: [successor_indices]}
                       å¦‚æœä¸º Noneï¼Œå‡è®¾æ˜¯çº¿æ€§åºåˆ—
        """
        self.operators = operators
        self.num_ops = len(operators)

        # æ„å»ºé‚»æ¥å…³ç³»
        if adjacency is None:
            # é»˜è®¤çº¿æ€§åºåˆ—
            self.adjacency = {i: [i+1] for i in range(self.num_ops - 1)}
            self.adjacency[self.num_ops - 1] = []
        else:
            self.adjacency = adjacency

        # æ„å»ºåå‘é‚»æ¥
        self.reverse_adj = {i: [] for i in range(self.num_ops)}
        for src, dsts in self.adjacency.items():
            for dst in dsts:
                if dst < self.num_ops:
                    self.reverse_adj[dst].append(src)

    def propagate_from(self, start_op: int, partition: Any) -> Dict[int, Any]:
        """
        ä»æŒ‡å®šç®—å­å¼€å§‹ä¼ æ’­åˆ†åŒºã€‚

        åªæœ‰å¸ƒå±€æ•æ„Ÿç®—å­æ‰èƒ½ä½œä¸ºä¼ æ’­èµ·ç‚¹ã€‚
        å¸ƒå±€ä¸æ•æ„Ÿç®—å­ä¼šé€ä¼ åˆ†åŒºã€‚

        Returns:
            Dict[op_idx, partition]: æ‰€æœ‰å¯ä»¥ä½¿ç”¨è¯¥åˆ†åŒºçš„ç®—å­
        """
        result = {start_op: partition}

        # å‘å‰ä¼ æ’­ï¼ˆä¸‹æ¸¸ï¼‰
        self._propagate_forward(start_op, partition, result)

        # å‘åä¼ æ’­ï¼ˆä¸Šæ¸¸ï¼‰
        self._propagate_backward(start_op, partition, result)

        return result

    def _propagate_forward(self, start_op: int, partition: Any,
                           result: Dict[int, Any]):
        """å‘ä¸‹æ¸¸ä¼ æ’­"""
        queue = deque([start_op])
        visited = {start_op}

        while queue:
            current = queue.popleft()

            for next_op in self.adjacency.get(current, []):
                if next_op in visited or next_op >= self.num_ops:
                    continue

                next_info = self.operators[next_op]

                # å¸ƒå±€ä¸æ•æ„Ÿç®—å­ï¼ˆæ— è§„çº¦ï¼‰å¯ä»¥é€ä¼ 
                if next_info.sensitivity == LayoutSensitivity.INSENSITIVE:
                    # æ£€æŸ¥å½¢çŠ¶å…¼å®¹æ€§
                    if self._shape_compatible(current, next_op):
                        result[next_op] = partition
                        visited.add(next_op)
                        queue.append(next_op)

    def _propagate_backward(self, start_op: int, partition: Any,
                            result: Dict[int, Any]):
        """å‘ä¸Šæ¸¸ä¼ æ’­"""
        queue = deque([start_op])
        visited = {start_op}

        while queue:
            current = queue.popleft()

            for prev_op in self.reverse_adj.get(current, []):
                if prev_op in visited:
                    continue

                prev_info = self.operators[prev_op]

                # å¸ƒå±€ä¸æ•æ„Ÿç®—å­ï¼ˆæ— è§„çº¦ï¼‰å¯ä»¥é€ä¼ 
                if prev_info.sensitivity == LayoutSensitivity.INSENSITIVE:
                    if self._shape_compatible(prev_op, current):
                        result[prev_op] = partition
                        visited.add(prev_op)
                        queue.append(prev_op)

    def _shape_compatible(self, src_op: int, dst_op: int) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªç®—å­çš„å½¢çŠ¶æ˜¯å¦å…¼å®¹"""
        src_info = self.operators[src_op]
        dst_info = self.operators[dst_op]

        # è¾“å‡ºå½¢çŠ¶ == è¾“å…¥å½¢çŠ¶
        return src_info.output_shape == dst_info.input_shape or \
            not src_info.output_shape or not dst_info.input_shape

    def find_propagation_groups(self) -> List[Set[int]]:
        """
        æ‰¾å‡ºæ‰€æœ‰å¯ä»¥å…±äº«åˆ†åŒºçš„ç®—å­ç»„ã€‚

        æ¯ä¸ªå¸ƒå±€æ•æ„Ÿç®—å­æ˜¯ä¸€ä¸ªç»„çš„"é”šç‚¹"ï¼Œä¸æ•æ„Ÿç®—å­é™„ç€åˆ°ç›¸é‚»çš„æ•æ„Ÿç®—å­ã€‚

        è§„åˆ™ï¼š
        1. æ¯ä¸ªæ•æ„Ÿç®—å­å½¢æˆä¸€ä¸ªç»„çš„æ ¸å¿ƒ
        2. ä¸æ•æ„Ÿç®—å­é™„ç€åˆ°å…¶å‰é©±æ•æ„Ÿç®—å­
        3. ç»„å†…çš„å±‚å…±äº«ç›¸åŒåˆ†åŒºï¼Œæ— éœ€é‡åˆ†å¸ƒ
        """
        groups = []
        assigned = set()

        # æ‰¾åˆ°æ‰€æœ‰æ•æ„Ÿç®—å­ä½œä¸ºé”šç‚¹
        sensitive_ops = self.get_sensitive_operators()

        for anchor in sensitive_ops:
            if anchor in assigned:
                continue

            # è¯¥ç»„åŒ…å«é”šç‚¹å’Œå¯è¾¾çš„ä¸æ•æ„Ÿç®—å­
            group = {anchor}
            assigned.add(anchor)

            # åªå‘å‰ä¼ æ’­ï¼ˆä¸‹æ¸¸ï¼‰ï¼Œé‡åˆ°æ•æ„Ÿç®—å­åœæ­¢
            queue = deque([anchor])
            visited = {anchor}

            while queue:
                current = queue.popleft()

                for next_op in self.adjacency.get(current, []):
                    if next_op in visited or next_op >= self.num_ops:
                        continue

                    next_info = self.operators[next_op]

                    # åªä¼ æ’­åˆ°ä¸æ•æ„Ÿç®—å­
                    if next_info.sensitivity == LayoutSensitivity.INSENSITIVE:
                        if self._shape_compatible(current, next_op):
                            group.add(next_op)
                            assigned.add(next_op)
                            visited.add(next_op)
                            queue.append(next_op)

            groups.append(group)

        # å¤„ç†æœªåˆ†é…çš„ä¸æ•æ„Ÿç®—å­ï¼ˆæ²¡æœ‰å‰é©±æ•æ„Ÿç®—å­çš„æƒ…å†µï¼‰
        for i in range(self.num_ops):
            if i not in assigned:
                groups.append({i})
                assigned.add(i)

        return groups

    def get_sensitive_operators(self) -> List[int]:
        """è·å–æ‰€æœ‰å¸ƒå±€æ•æ„Ÿç®—å­çš„ç´¢å¼•"""
        return [i for i, op in enumerate(self.operators)
                if op.sensitivity == LayoutSensitivity.SENSITIVE]

    def get_insensitive_operators(self) -> List[int]:
        """è·å–æ‰€æœ‰å¸ƒå±€ä¸æ•æ„Ÿç®—å­çš„ç´¢å¼•"""
        return [i for i, op in enumerate(self.operators)
                if op.sensitivity == LayoutSensitivity.INSENSITIVE]


def analyze_network_sensitivity(layers: List[Any]) -> List[OperatorInfo]:
    """
    åˆ†ææ•´ä¸ªç½‘ç»œçš„å¸ƒå±€æ•æ„Ÿæ€§ã€‚

    Args:
        layers: å±‚å¯¹è±¡åˆ—è¡¨

    Returns:
        æ¯å±‚çš„ç®—å­ä¿¡æ¯ï¼ˆåŒ…å«è§„çº¦åˆ†æç»“æœï¼‰
    """
    return [ReductionAnalyzer.analyze(layer) for layer in layers]


def find_layout_propagation_groups(layers: List[Any],
                                   adjacency: Dict[int, List[int]] = None) -> List[Set[int]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰¾å‡ºå¯ä»¥å…±äº«åˆ†åŒºçš„å±‚ç»„ã€‚

    Args:
        layers: å±‚å¯¹è±¡åˆ—è¡¨
        adjacency: é‚»æ¥è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        å±‚ç»„åˆ—è¡¨ï¼Œæ¯ç»„å†…çš„å±‚å¯ä»¥å…±äº«åˆ†åŒº
    """
    op_infos = analyze_network_sensitivity(layers)
    propagator = LayoutPropagator(op_infos, adjacency)
    return propagator.find_propagation_groups()


# ============================================================================
# æ¼”ç¤º
# ============================================================================

def demo():
    """æ¼”ç¤ºåŸºäºè§„çº¦åˆ†æçš„ Layout Propagation"""

    print("=" * 70)
    print("Layout Propagation (åŸºäºè§„çº¦åˆ†æ)")
    print("=" * 70)

    print("""
æ ¸å¿ƒåŸåˆ™:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æœ‰è§„çº¦è®¡ç®— â†’ å¸ƒå±€æ•æ„Ÿï¼ˆSensitiveï¼‰   â†’ æ˜¯åˆ†åŒºè¾¹ç•Œ
  æ— è§„çº¦è®¡ç®— â†’ å¸ƒå±€ä¸æ•æ„Ÿï¼ˆInsensitiveï¼‰â†’ å¯é€ä¼ åˆ†åŒº
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ç®—å­è§„çº¦åˆ†æ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç®—å­        â”‚ æœ‰è§„çº¦?     â”‚ è§„çº¦ç»´åº¦      â”‚ å¸ƒå±€æ•æ„Ÿ?           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv        â”‚ âœ“           â”‚ C, R, S      â”‚ âœ“ Sensitive         â”‚
â”‚ FC/MatMul   â”‚ âœ“           â”‚ K            â”‚ âœ“ Sensitive         â”‚
â”‚ Pool        â”‚ âœ“           â”‚ H, W (çª—å£)  â”‚ âœ“ Sensitive         â”‚
â”‚ Softmax     â”‚ âœ“           â”‚ C            â”‚ âœ“ Sensitive         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ReLU        â”‚ âœ—           â”‚ -            â”‚ âœ— Insensitive       â”‚
â”‚ Add/Mul     â”‚ âœ—           â”‚ -            â”‚ âœ— Insensitive       â”‚
â”‚ BatchNorm*  â”‚ âœ—           â”‚ -            â”‚ âœ— Insensitive       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* BatchNorm æ¨ç†æ—¶æ— è§„çº¦
""")

    # åˆ›å»ºæ¨¡æ‹Ÿç½‘ç»œ
    class MockLayer:
        def __init__(self, name, layer_type, nifm, nofm, hofm, wofm=None, hfil=1):
            self.name = name
            self.nifm = nifm
            self.nofm = nofm
            self.hofm = hofm
            self.wofm = wofm or hofm
            self.hifm = hofm  # ç®€åŒ–ï¼šå‡è®¾è¾“å…¥è¾“å‡ºç©ºé—´ç›¸åŒ
            self.wifm = wofm or hofm
            self.hfil = hfil
            self._type = layer_type

        @property
        def __class__(self):
            class FakeClass:
                pass
            FakeClass.__name__ = self._type
            return FakeClass

    # VGG-style ç½‘ç»œç‰‡æ®µ
    layers = [
        MockLayer('conv1', 'ConvLayer', 3, 64, 224, hfil=3),
        MockLayer('bn1', 'BatchNorm', 64, 64, 224),
        MockLayer('relu1', 'ReLU', 64, 64, 224),
        MockLayer('conv2', 'ConvLayer', 64, 64, 224, hfil=3),
        MockLayer('bn2', 'BatchNorm', 64, 64, 224),
        MockLayer('relu2', 'ReLU', 64, 64, 224),
        MockLayer('pool1', 'MaxPool', 64, 64, 112),
        MockLayer('conv3', 'ConvLayer', 64, 128, 112, hfil=3),
        MockLayer('relu3', 'ReLU', 128, 128, 112),
        MockLayer('fc1', 'FCLayer', 128, 1000, 1),
    ]

    print("\nç¤ºä¾‹ç½‘ç»œ:")
    print("-" * 70)

    # åˆ†ææ¯å±‚
    op_infos = analyze_network_sensitivity(layers)

    for i, (layer, op_info) in enumerate(zip(layers, op_infos)):
        reduction_str = f"è§„çº¦ç»´åº¦: {op_info.reduction_dims}" if op_info.has_reduction else "æ— è§„çº¦"
        sensitivity = "ğŸ”´ æ•æ„Ÿ" if op_info.sensitivity == LayoutSensitivity.SENSITIVE else "ğŸŸ¢ ä¸æ•æ„Ÿ"
        print(
            f"  {i}: {layer.name:10s} | {op_info.op_type:12s} | {reduction_str:20s} | {sensitivity}")

    # ä¼ æ’­åˆ†æ
    propagator = LayoutPropagator(op_infos)
    groups = propagator.find_propagation_groups()

    print("\n" + "-" * 70)
    print("ä¼ æ’­ç»„ (å…±äº«åˆ†åŒº):")
    print("-" * 70)

    for i, group in enumerate(groups):
        layer_names = [layers[idx].name for idx in sorted(group)]
        if len(group) > 1:
            print(f"  ç»„ {i+1}: {' â†’ '.join(layer_names)}")
            print(f"         â””â”€ è¿™äº›å±‚å¯ä»¥é€ä¼ åˆ†åŒºï¼Œæ— éœ€é‡åˆ†å¸ƒ")
        else:
            idx = list(group)[0]
            sensitivity = op_infos[idx].sensitivity.value
            print(f"  ç»„ {i+1}: {layer_names[0]} ({sensitivity})")

    # ç»Ÿè®¡
    sensitive_ops = propagator.get_sensitive_operators()
    insensitive_ops = propagator.get_insensitive_operators()

    print("\n" + "-" * 70)
    print("ç»Ÿè®¡:")
    print("-" * 70)
    print(f"  å¸ƒå±€æ•æ„Ÿç®—å­ (æœ‰è§„çº¦): {len(sensitive_ops)} ä¸ª")
    print(f"    â†’ è¿™äº›æ˜¯åˆ†åŒºå†³ç­–ç‚¹")
    print(f"  å¸ƒå±€ä¸æ•æ„Ÿç®—å­ (æ— è§„çº¦): {len(insensitive_ops)} ä¸ª")
    print(f"    â†’ è¿™äº›å¯ä»¥é€ä¼ ï¼Œå‡å°‘ {len(insensitive_ops)} ä¸ªåˆ†åŒºå˜é‡")

    print("\n" + "=" * 70)
    print("ä¼˜åŒ–æ•ˆæœ:")
    print("=" * 70)
    print(f"""
åŸå§‹é—®é¢˜: {len(layers)} ä¸ªç®—å­ï¼Œæ¯ä¸ªç‹¬ç«‹å†³ç­–åˆ†åŒº
ä¼˜åŒ–å:   {len(sensitive_ops)} ä¸ªåˆ†åŒºå†³ç­–ç‚¹

å‡å°‘å†³ç­–å˜é‡: {len(layers) - len(sensitive_ops)} ä¸ª ({100*(len(layers)-len(sensitive_ops))/len(layers):.1f}%)
""")


if __name__ == '__main__':
    demo()
